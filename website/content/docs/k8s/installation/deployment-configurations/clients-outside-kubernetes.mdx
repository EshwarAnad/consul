---
layout: docs
page_title: Consul Clients Outside of Kubernetes - Kubernetes
sidebar_title: Consul Clients Outside Kubernetes
description: >-
  Consul clients running on non-Kubernetes nodes can join a Consul cluster
  running within Kubernetes.
---

# Consul Clients Outside Kubernetes

Consul clients running on non-Kubernetes nodes can join a Consul cluster running within Kubernetes.

## Networking

Within one datacenter, Consul typically requires a fully connected
[network](/docs/architecture). This means the IPs of every client and server
agent should be routeable by every other client and server agent in the
datacenter. Clients need to be able to gossip with every other agent and make
RPC calls to servers. Servers need to be able to gossip with every other
agent.

-> **Consul Enterprise customers** may use [network
segments](/docs/enterprise/network-segments) to enable non-fully-connected
topologies. However, out-of-cluster nodes must still be able to communicate
with the server pod or host IP addresses.

## Auto-join
The recommended way to join a cluster running within Kubernetes is to
use the ["k8s" cloud auto-join provider](/docs/agent/cloud-auto-join#kubernetes-k8s).

The auto-join provider dynamically discovers IP addresses to join using
the Kubernetes API. It authenticates with Kubernetes using a standard
`kubeconfig` file. This works with all major hosted Kubernetes offerings
as well as self-hosted installations. The token in the `kubeconfig` file
needs to have permissions to list pods in the namespace where Consul servers
are deployed.

The auto-join string below will join a Consul server cluster that is
started using the [official Helm chart](/docs/k8s/helm):

```shell-session
$ consul agent -retry-join 'provider=k8s label_selector="app=consul,component=server"'
```

By default, Consul will join the default gossip port. Pods may set an
annotation `consul.hashicorp.com/auto-join-port` to an integer value or
a named port to specify the port for the auto-join to return. This enables
different pods to have different exposed ports.

### Auto-join on the Pod network
In the default Consul Helm chart installation, the Consul clients and servers
are deployed as pods and routeable only via their `podIP:containerPort` for
gossip/RPC. This would mean any external client agents joining would need to
be able to have connectivity to those pod IPs.

Given you have the [official Helm chart](/docs/k8s/helm) installed with the default values, do the following to join an external client agent. 

1. Make sure the pod IPs of the clients and servers in Kubernetes are routeable from the VM and that the VM IP (`$ADVERTISE_IP`) is routeable from the pods, adding any firewall rules necessary to allow traffic on the gossip and RPC ports.

2. Make sure you have the `kubeconfig` file for the Kubernetes cluster in `$HOME/.kube/config`.

2.  On the external VM, run: 
```shell-session
consul agent \
  -advertise="$ADVERTISE_IP" \
  -retry-join='provider=k8s label_selector="app=consul,component=server"'
  -bind=0.0.0.0 \
  -client=0.0.0.0 \
  -hcl='leave_on_terminate = true' \
  -hcl='ports { grpc = 8502 }' \
  -config-dir=$CONFIG_DIR \
  -datacenter=$DATACENTER \
  -data-dir=$DATA_DIR \
```

3. Check if the join was successful by running `consul members`

### Auto-join on the Host network
If you don't have connectivity to the `podIP:containerPort` for clients and servers deployed in Kubernetes, you have the option to expose the clients and servers on the host network. 

1. Install the [official Helm chart](/docs/k8s/helm) with the following values:
```yaml
client:
  exposeGossipPorts: true # exposes client gossip ports as hostPorts
server:
  exposeGossipAndRPCPorts: true # exposes the server gossip and RPC ports as hostPorts
  ports:
    # Configures the server gossip port
    serflan:
      # Note that this needs to be different than 8301, to avoid conflicting with the client gossip hostPort
      port: 9301 
```
 This will expose the client gossip ports, the server gossip ports and the server RPC port at `hostIP:hostPort`. Note that `hostIP` is the **internal** IP of the VM that the client/server pods are deployed on.

2. Make sure the pod IPs of the clients and servers in Kubernetes are routeable from the VM and that the VM IP (`$ADVERTISE_IP`) is routeable from the pods, adding any firewall rules necessary to allow traffic on the gossip and RPC ports for the K8s Nodes and the external VM.

3. Make sure you have the `kubeconfig` file for the Kubernetes cluster in `$HOME/.kube/config`.

4. On the external VM, run (note the addition of `host_network=true` in the retry-join argument): 
```shell-session
consul agent \
  -advertise="$ADVERTISE_IP" \
  -retry-join='provider=k8s host_network=true label_selector="app=consul,component=server"'
  -bind=0.0.0.0 \
  -client=0.0.0.0 \
  -hcl='leave_on_terminate = true' \
  -hcl='ports { grpc = 8502 }' \
  -config-dir=$CONFIG_DIR \
  -datacenter=$DATACENTER \
  -data-dir=$DATA_DIR \
``` 
3. Check if the join was successful by running `consul members`

## Manual join
If you are unable to use auto-join, you can also follow the instructions in either of the auto-join sections but instead of using a `provider` key in the `-retry-join` flag, you would need to hardcode the IP and gossip port of every consul server, e.g: `-retry-join=$CONSUL_SERVER_IP:$SERVER_SERFLAN_PORT`.   
